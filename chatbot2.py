# -*- coding: utf-8 -*-
"""chatbot2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d72G5KRGz0Q6ZeHNywaKqfphaitQAHDb

# Import Libraries
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score,confusion_matrix, precision_score, recall_score, auc
from scipy import interp
from itertools import cycle
import json, pickle
import tensorflow as tf
from tensorflow.keras import models, layers
from tensorflow.keras import optimizers
import matplotlib.pyplot as plt
import nltk
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('wordnet')

"""# Load Dataset"""

data = open('intents.json').read()
intents = json.loads(data)

"""# Preprocess Data - Lematization and Tokenization"""

documents = []
words = []
classes = []
punctuations = ['?', '!', ',']

for intent in intents['intents']:
  for pattern in intent['patterns']:
    # tokenize words (patterns) and add to words list
    tokenized_words = nltk.word_tokenize(pattern)
    words.extend(tokenized_words)

    # append tokenized words and tags to documents lists
    documents.append((tokenized_words, intent['tag']))
    if intent['tag'] not in classes:
      classes.append(intent['tag'])

words_new = []
wordnet_lemmatizer = WordNetLemmatizer()
for tokenized_words in words:
  if tokenized_words not in punctuations:
    lemma = wordnet_lemmatizer.lemmatize(tokenized_words.lower())
    words_new.append(lemma)
words_new = sorted(list(set(words_new)))
classes = sorted(list(set(classes)))

words_new

classes

"""# Save the Word list and Question Categories (Classes) as pickle files"""

with open('words.pkl', 'wb') as outfile:
  pickle.dump(words_new, outfile)

with open('classes.pkl', 'wb') as outfile:
  pickle.dump(classes, outfile)

"""# Create Input and Target Array"""

data_set = []
input_array = np.zeros((len(documents), len(words_new)))
target_array = np.zeros((len(documents), len(classes)))
doc_count = 0
for document in documents:
  pattern_words = document[0]
  target_array[doc_count, classes.index(document[1])] = 1

  lemmatized_pattern_words = []
  for pattern_word in pattern_words:
    lemma = wordnet_lemmatizer.lemmatize(pattern_word.lower())
    lemmatized_pattern_words.append(lemma)

  for id, word in enumerate(words_new):
    if word in lemmatized_pattern_words:
      input_array[doc_count, id] = 1

  doc_count += 1

"""# Train/Test Split"""

X_train, X_test, y_train, y_test = train_test_split(input_array, target_array, random_state=42)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

"""# Model Training"""

model = models.Sequential()
model.add(layers.Dense(128, activation='relu', input_shape=X_train[1,:].shape))
model.add(layers.Dropout(rate=0.5))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(rate=0.5))
model.add(layers.Dense(y_train.shape[1], activation='softmax'))

model.summary()
print('\n')
# training
#Using callbacks for training control
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if logs.get('val_accuracy')>0.8:
      print('Treshold accuracy reached, cancelling training....\n')
      self.model.stop_training = True

tf.random.set_seed(42)
sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
my_calback_object = myCallback()
model.compile(optimizer = sgd, loss='categorical_crossentropy', metrics = ['accuracy'])
history = model.fit(X_train, y_train, epochs=200, batch_size = X_train.shape[0], validation_data=(X_test, y_test), verbose=True, callbacks=[my_calback_object])

"""# Visualize Model Performance"""

fig, ax = plt.subplots()
ax.plot(history.history['accuracy'], label = 'training accuracy')
ax.plot(history.history['val_accuracy'], label = 'validation accuracy')
ax.set_xlabel('Epochs')
ax.set_ylabel('Accuracy')
ax2 = ax.twinx()
ax2.plot(history.history['loss'], label = 'training loss')
ax2.plot(history.history['val_loss'], label = 'validation loss')
ax2.set_ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.title('model performance')
plt.show()

"""# Evaluate Model Performance"""

y_predict = model.predict(X_test)
accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_predict, axis=1))
print(f"Accuracy is {round(accuracy, 2) * 100}%\n")
_precision_score_ = precision_score(np.argmax(y_test, axis=1), np.argmax(y_predict, axis=1), average='micro')
print(f"Precision is {round(_precision_score_, 2) * 100}%\n")
_recall_score_ = precision_score(np.argmax(y_test, axis=1), np.argmax(y_predict, axis=1), average='micro')
print(f"Recall is {round(_recall_score_, 2) * 100}%\n")
# confusion_matrix_df = pd.DataFrame(data = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_predict, axis=1)), index=["negative_actual", "positive_actual"], columns=["negative_predicted", "positive_predicted"])
confusion_matrix_df = pd.DataFrame(data = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_predict, axis=1)))
confusion_matrix_df

# Compute ROC curves
n_classes = 9
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_predict[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_predict.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure(figsize=(12, 10))
lw = 2
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic to multi-class')
plt.legend(loc="lower right")
plt.show()

"""# Save Model"""

model.save('bot_model.h5')